 % use the "wcp" class option for workshop and conference
 % proceedings
 %\documentclass[gray]{jmlr} % test grayscale version
 %\documentclass[tablecaption=bottom]{jmlr}% journal article
 \documentclass[tablecaption=bottom,wcp]{jmlr} % W&CP article

 % The following packages will be automatically loaded:
 % amsmath, amssymb, natbib, graphicx, url, algorithm2e

 %\usepackage{rotating}% for sideways figures and tables
 %\usepackage{longtable}% for long tables

 % The booktabs package is used by this sample document
 % (it provides \toprule, \midrule and \bottomrule).
 % Remove the next line if you don't require it.
\usepackage{booktabs}
 % The siunitx package is used by this sample document
 % to align numbers in a column by their decimal point.
 % Remove the next line if you don't require it.
\usepackage[load-configurations=version-1]{siunitx} % newer version
\usepackage{color}
 %\usepackage{siunitx}

 % The following command is just for this sample document:
\newcommand{\cs}[1]{\texttt{\char`\\#1}}% remove this in your real article

\newcommand{\TODO}[1]{\textcolor{red}{#1}}
 % Define an unnumbered theorem just for this sample document for
 % illustrative purposes:
\theorembodyfont{\upshape}
\theoremheaderfont{\scshape}
\theorempostheader{:}
\theoremsep{\newline}
\newtheorem*{note}{Note}

 % change the arguments, as appropriate, in the following:
\jmlrvolume{1}
\jmlryear{20XX}
\jmlrsubmitted{submission date}
\jmlrpublished{publication date}
\jmlrworkshop{workshop title} % W&CP title

 % The optional argument of \title is used in the header
\title[SUNNY-OASC]{SUNNY with Algorithm Configuration}


 % Two authors with the same address
  \author{\Name{Tong Liu} \Email{t.liu@unibo.it}\\
   \Name{Roberto Amadini} \Email{roberto.amadini@unimelb.edu.au}\\
   \Name{Jacopo Mauro} \Email{mauro.jacopo@gmail.com}\\
    }



 % Authors with different addresses:
 % \author{\Name{Author Name1} \Email{abc@sample.com}\\
 % \addr Address 1
 % \AND
 % \Name{Author Name2} \Email{xyz@sample.com}\\
 % \addr Address 2
 %}


 %\editors{Editor One and Editor Two}% for multiple editors

\begin{document}

\maketitle



\section{Description}

SUNNY-OASC is an per instance algorithm scheduling strategy based on K-NN 
technique.
SUNNY-OASC is an extension of 
the SUNNY-AS tool (see \cite{ictai_paper,sunnyas}) with some 
ideas borrowed from the work of  
\cite{DBLP:conf/lion/LindauerBH16}. 
% and \cite{Kohavi97wrappersfor}.

% In particular, as \cite{DBLP:conf/lion/LindauerBH16} show, the improvement 
% in performance w.r.t. SUNNY-AS is mainly due to  has demonstrated that how a 
% training phase, by studying the value $K$ and the number of solvers, can boost 
% SUNNY performance. However, we believe that the number of solvers is a trivial 
% option to configure, because, in our original version, SUNNY employs less 
% solvers as reported. 

\subsection{Execution modalities}

SUNNY-OASC has two execution modalities: \texttt{autok} and 
\texttt{fkvar}. 

\begin{itemize}
  \item The \texttt{autok} is a variant of T-SUNNY as defined by 
\cite{DBLP:conf/lion/LindauerBH16} where SUNNY-AS has been improved training 
also on the size of the value of the neighborhood 
$K$.\footnote{\texttt{autok} is slightly different than T-SUNNY since the 
reimplementation of T-SUNNY used a different algorithm to select the 
solvers to use. To chose the solvers we used the original SUNNY-AS 
algorithm.}
  \item The \texttt{fkvar} instead trains for the neighborhood value $K$ and 
the subset of features to consider by using a wrapper method 
\cite{Kohavi97wrappersfor}. SUNNY is used as the evaluator and a 
greedy forward selection is adopted to select the subset of features for 
computing the neighborhood.
% 
The selection cycle is defined as follow: 
the unselected feature set is considered and we pick one feature at the time 
adding it to the selected features set (initially empty) to form a test 
feature set. By tuning also the value k, SUNNY calculates the best par10 score 
that it can achieve with the test feature set. Based on the outcome, a new 
feature is added until the performance decrease or we have performed a given 
number of evaluations. In the end, \texttt{fkvar} 
produces a combination of features and a value $K$ for which SUNNY performs the 
best on training data.
\end{itemize}
 

\subsection{Representative instances}
For performance reasons, SUNNY-OASC is not used on all the instances available 
but only on some selected ones.
The representative instances use for the training are selected as follow: i) 
SUNNY-OASC first associates each instance to a solver that solves it in the 
least time, ii) for each solver instances are ordered from hard to easy in terms 
of runtime, ii) for each solvers one instance at the time is picked until a 
global limit on the number of representative instances is reached.

\subsection{Parameters for the Challenge}
The experiments performed by \cite{ictai_paper} suggest that a handful subset 
of features (e.g., 5 or less) is often enough for SUNNY to obtain a 
competitive performance. For this reason, in \texttt{fkvar} we fixed 
5 as the amount of feature to select. In order to guarantee an acceptable 
execution runtime, for the \texttt{fkvar} approach, we have chosen to 
consider only $1500$ and not more instances to be included in the 
representative instance set. We also fixed the interval of $K$ as [3,30]. 
% \footnote{Note that with scenarios with around 5.000 instances, the training 
% may take a couple of days for training. Besides, the interval [3,30] would 
% cover most of the useful K values. }.

% \TODO{Jac: non capisco questa frase e pertanto la commento. In addition, in the 
% submitted \texttt{fkvar} version, when evaluating feature relevance, the par10 
% is calculated by averaging par10 of the best k instead of a single best k.}

When \texttt{fkvar} is executed, we also run 
\texttt{autok} with $K \in [3,80]$ as a backup. If SUNNY runs better with the 
entire feature set, we then use the solution produced by \texttt{autok}.

For the \texttt{autok} version submitted, differently to the one used as a 
backup when running SUNNY-OASC in the \texttt{fkvar} modality, we consider the 
full training set as effective training data (i.e., more than 1500 
instances are used to train if available). 

\section{Setup Instruction}

The source code of SUNNY-OACS is available at \cite{sunnyoasc} and requires 
Python v2. There are five folders: `data' and `results' contain oasc-challenge 
data and solution results respectively, `src' contains the original SUNNY-AS 
scripts from \cite{sunnyas}, `oasc' contains scripts that coordinate those in 
`src' for training and testing, the folder `main' contains the scripts that 
automatically call `oasc' for the different execution modalities. 

The program runs training and testing in sequence. Let us take \texttt{autok} 
approach as execution example. To run it, in the folder `main' the 
command ``sh make\_oasc\_tasks.sh $<$ tasks.txt'' must be used to create 
the tasks. Then the train cab be done by running ``sh oasc\_train.sh 
run\_autok tasks.txt''. After training, the test is run by ``sh 
make\_oasc\_tasks.sh $<$ tasks.txt'' and later by ``sh oasc\_test.sh autok 
tasks.txt''.

To run \texttt{fkvar} it is sufficient to replace 
literally \texttt{autok} by \texttt{fkvar} in the previous commands. 

\bibliography{reference}

% \appendix

\end{document}
