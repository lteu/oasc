 % use the "wcp" class option for workshop and conference
 % proceedings
 %\documentclass[gray]{jmlr} % test grayscale version
 %\documentclass[tablecaption=bottom]{jmlr}% journal article
 \documentclass[tablecaption=bottom,wcp]{jmlr} % W&CP article

 % The following packages will be automatically loaded:
 % amsmath, amssymb, natbib, graphicx, url, algorithm2e

 %\usepackage{rotating}% for sideways figures and tables
 %\usepackage{longtable}% for long tables

 % The booktabs package is used by this sample document
 % (it provides \toprule, \midrule and \bottomrule).
 % Remove the next line if you don't require it.
\usepackage{booktabs}
 % The siunitx package is used by this sample document
 % to align numbers in a column by their decimal point.
 % Remove the next line if you don't require it.
\usepackage[load-configurations=version-1]{siunitx} % newer version
\usepackage{color}
 %\usepackage{siunitx}

 % The following command is just for this sample document:
\newcommand{\cs}[1]{\texttt{\char`\\#1}}% remove this in your real article

\newcommand{\TODO}[1]{\textcolor{red}{#1}}
 % Define an unnumbered theorem just for this sample document for
 % illustrative purposes:
\theorembodyfont{\upshape}
\theoremheaderfont{\scshape}
\theorempostheader{:}
\theoremsep{\newline}
\newtheorem*{note}{Note}

 % change the arguments, as appropriate, in the following:
\jmlrvolume{1}
\jmlryear{2017}
\jmlrsubmitted{submission date}
\jmlrpublished{publication date}
\jmlrworkshop{OASC Challenge} % W&CP title

 % The optional argument of \title is used in the header
\title[SUNNY-OASC]{SUNNY with Algorithm Configuration}




 % Two authors with the same address
  % \author{\Name{Tong Liu} \Email{t.liu@unibo.it}\\
  %  \Name{Roberto Amadini} \Email{roberto.amadini@unimelb.edu.au}\\
  %  \Name{Jacopo Mauro} \Email{mauro.jacopo@gmail.com}\\
  %   }



 % Authors with different addresses:
 \author{\Name{Tong Liu} \Email{t.liu@unibo.it}\\
 \addr University of Bologna, Italy 
 \AND
 \Name{Roberto Amadini} \Email{roberto.amadini@unimelb.edu.au}\\
 \addr University of Melbourne, Australia
 \AND
 \Name{Jacopo Mauro} \Email{mauro.jacopo@gmail.com}\\
 \addr University of Oslo, Norway
 }


 %\editors{Editor One and Editor Two}% for multiple editors

\begin{document}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
The SUNNY algorithm is a portfolio technique originally tailored for 
Constraint Satisfaction Problems (CSPs). SUNNY allows to select a set of 
solvers to be run on a given CSP, and was proven to be effective in the 
MiniZinc Challenge, i.e., the yearly international competition 
for CP solvers. In 2015, SUNNY was compared with other solver selectors in the 
first ICON Challenge on algorithm selection with less satisfactory performance.
In this paper we briefly describe the new version of the SUNNY approach for 
algorithm selection, that was submitted to the first Open Algorithm 
Selection Challenge.
\end{abstract}


\section{SUNNY-OASC}

% \subsection{SUNNY}

SUNNY is a per instance algorithm scheduling strategy based on $k$-NN algorithm.
% The main aim of SUNNY is to minimize the runtime for solving problem instances. 
Roughly speaking, for each test instance SUNNY selects
$k$ training instances which are similar to the test instance
in terms of Euclidean Distance (on instance features). Based on the selected instances,
SUNNY generates a schedule of solvers that maximize the number of
instances solved by the selected solvers. 
Then, a time slot proportional to the fraction of solved instances is
assigned to each solver. Finally, the proposed solvers are ordered
according to the average solving time on the selected instances.

In the OASC challenge, there were also several scenarios 
where the goal was maximization instead of satisfaction.
For these problems we used an experimental approach:
straightforwardly, after selecting the $k$ neighbourhood instances, 
we picked only one solver which achieves the highest accumulated solution score 
on the $k$ instances.
Note that this is not the default approach used by SUNNY for constraint 
optimization problems~\citep{sunnycp2,paper_amai}.

For a detailed description of the SUNNY approach we refer the interested 
reader to \cite{sunny,sunnycp2,paper_amai}.
% In the tool SUNNY-AS (see ),
% we selected a subset of solvers, which solve 
% the $k$ instances as least time as possible.
In the following we present SUNNY-OASC, an extension of the
original SUNNY algorithm that enables the configuration of the neighborhood 
size $k$ 
(an idea borrowed from \cite{DBLP:conf/lion/LindauerBH16}) and a 
wrapper-based feature selection.

% and \cite{Kohavi97wrappersfor}.

% In particular, as \cite{DBLP:conf/lion/LindauerBH16} show, the improvement 
% in performance w.r.t. SUNNY-AS is mainly due to  has demonstrated that how a 
% training phase, by studying the value $k$ and the number of solvers, can boost 
% SUNNY performance. However, we believe that the number of solvers is a trivial 
% option to configure, because, in our original version, SUNNY employs less 
% solvers as reported. 

\subsection{Execution modalities}

SUNNY-OASC has two execution modalities: \texttt{autok} and 
\texttt{fkvar}.

\begin{itemize}
\item The \texttt{autok} approach is a variant of T-SUNNY \citep{DBLP:conf/lion/LindauerBH16}, 
an improvement of SUNNY-AS that trains also on the size of the neighborhood 
$k$. \texttt{autok} is slightly different than T-SUNNY since the 
reimplementation of T-SUNNY used a different algorithm to select the 
solvers to use. To choose the solvers we used the original SUNNY-AS 
algorithm instead.

%   \item The \texttt{autok} is a variant of T-SUNNY as defined by 
% \cite{DBLP:conf/lion/LindauerBH16} where SUNNY-AS has been improved by training 
% also on the size of the neighborhood 
% $k$.\footnote{\texttt{autok} is slightly different than T-SUNNY since the 
% reimplementation of T-SUNNY used a different algorithm to select the 
% solvers to use. To choose the solvers we used the original SUNNY-AS 
% algorithm.}

  \item The \texttt{fkvar} trains both on the neighborhood size and 
the subset of features to consider by using a wrapper method 
\citep{Kohavi97wrappersfor}. SUNNY is used as the evaluator and a 
greedy forward selection is adopted to select the subset of features for 
computing the neighborhood.
% 
The selection procedure is defined as follow: 
the unselected feature set is considered and we pick one feature at time, 
adding it to the selected feature set (initially empty) to form a test 
feature set. By also tuning the value k, SUNNY calculates the best PAR10 score 
that it can achieve with the test feature set. Based on the outcome, a new 
feature is added until the performance decrease or we have performed a given 
number of evaluations. Finally, \texttt{fkvar} 
produces a combination of features and a value $k$ for which SUNNY performs the 
best on training data.

When the selection procedure ends, we also run SUNNY in %as a backup the tool in 
\texttt{autok} modality. This is helpful
for scenarios where the whole set of features is more relevant than the 
feature set selected by using the wrapper filtering method. 
In these cases, we simply use the setting computed by \texttt{autok}.
\end{itemize}
 

\subsection{Representative instances}
Since training is computationally expensive and may take a long time,\footnote{On a PC 
with Intel Core i5 and 8 GB RAM running Ubuntu, 
training only one fold out of 10 of the ASLib scenario PROTUS ($4,000$ 
instances) would take for instance 35 hours using the \texttt{fkvar} approach.} 
SUNNY-OASC is not used on all the instances available but only on some selected 
ones.
The representative instances used for the training are selected as follows: (i) 
SUNNY-OASC first associates to each training instance the fastest solver for solving 
it, according to the training set; (ii) for each solver, instances are 
ordered from hard to easy in terms of runtime;
(iii) for each solver, one instance at a time is picked until a 
global limit on the number of representative instances is reached.

\subsection{Parameters for the Challenge}
\cite{bischl2016aslib} and \cite{ictai_paper} noted that a handful 
subset 
of features (e.g., 5 or less) is often enough for SUNNY to obtain  
competitive performance. For this reason, in \texttt{fkvar} we fixed 
5 as the number of feature to select. In order to guarantee an acceptable 
execution runtime, for the \texttt{fkvar} approach we 
consider only $1500$ instances to be included in the 
representative instance set. We also fixed $k$ to vary between 3 and 30. 
% \footnote{Note that with scenarios with around 5.000 instances, the training 
% may take a couple of days for training. Besides, the interval [3,30] would 
% cover most of the useful K values. }.

% \TODO{Jac: non capisco questa frase e pertanto la commento. In addition, in the 
% submitted \texttt{fkvar} version, when evaluating feature relevance, the par10 
% is calculated by averaging par10 of the best k instead of a single best k.}

% \TODO{Tong: infatti, la descrizione chiede 'brief description' quindi possiamo
% anche lasciare questo pezzo commentato. Pero' per spiegare meglio,
% faccio un esempio, par10 per K in [1,5] con feature f1 e' S =(3,2,3,5,1),
% utility(f1) = min(S) = min(3,2,3,5,1) = 1 
% invece, questa proposta del fkvar-3, e' utility(f1) = (1+2+3)/3 = 2 dove, 
% 1,2,3 sono i primi 3 par10 piu' piccoli in S.
% questo approccio aiuta sunny-as in qualche modo di evitare di staccare in local optima}

When \texttt{fkvar} is executed, we also run 
\texttt{autok} with $k \in [3,80]$ as a backup. If SUNNY runs better with the 
entire feature set, we then use the result produced by \texttt{autok}.

For the \texttt{autok} version submitted, different to the one used when 
running SUNNY-OASC in the \texttt{fkvar} modality, we consider the 
full training set as effective training data. 
%(i.e., more than 1500 instances are used for the training process if available). 

\section{Setup Instructions}

The source code of SUNNY-OACS is available at \cite{sunnyoasc} and requires 
Python v2. There are five folders: `data' and `results' contain oasc-challenge 
data and solution results respectively, `src' contains the original SUNNY-AS 
scripts from \cite{sunnyas}, `oasc' contains scripts that coordinate those in 
`src' for training and testing, the folder `main' contains the scripts that 
automatically call `oasc' for the different execution modalities. 

The program runs training and testing in sequence. Let us take \texttt{autok} 
approach as execution example. To run it, in the folder `main' the 
command \texttt{sh make\_oasc\_tasks.sh $>$ tasks.txt} must be used to create 
the tasks. Then the training can be done by running \texttt{sh oasc\_train.sh 
run\_autok tasks.txt}. The test is performed by running \texttt{sh 
make\_oasc\_tasks.sh $>$ tasks.txt} followed by \texttt{sh oasc\_test.sh autok 
tasks.txt}.

To run \texttt{fkvar} it is enough to replace 
 \texttt{autok} by \texttt{fkvar} in the above commands. 

\newpage 

\bibliography{reference}

% \appendix

\end{document}
